{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import random\n",
    "\n",
    "from time import time\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "from pyspark.sql import SQLContext\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    " \n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    " \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildArray(listOfIndices):\n",
    "    returnVal = np.zeros(f)\n",
    "\n",
    "    for index in listOfIndices:\n",
    "        returnVal[index] = returnVal[index] + 1\n",
    "\n",
    "    mysum = np.sum(returnVal)\n",
    "\n",
    "    returnVal = np.divide(returnVal, mysum)\n",
    "\n",
    "    return returnVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict1(x):\n",
    "    return int(model1.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "f = 20000\n",
    "sc = SparkContext(appName=\"task0\")\n",
    "# Pages = sc.textFile('winemag-data-130k-v2.csv')\n",
    "spark = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranfer_label(x):\n",
    "    if x < 88:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Postions in our Feature Matrix. Last 20 words in 20k positions:  [('ultragreen', 19999), ('desiccation', 19998), ('entertainment', 19997), ('repeating', 19996), ('pfaffenberg', 19995), ('overripenes', 19994), ('essentials', 19993), ('reduces', 19992), ('enwrapping', 19991), ('animo', 19990), ('parmalee', 19989), ('parnac', 19988), ('conservatively', 19987), ('threadbare', 19986), ('ion', 19985), ('santom', 19984), ('gods', 19983), ('vosges', 19982), ('nuns', 19981), ('bitner', 19980)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[31] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df= spark.read.csv('winemag-data-130k-v2.csv',header=True)\n",
    "\n",
    "dfs = pd.DataFrame(df.toPandas())\n",
    "dfs.dropna(subset=['_c0','description', 'designation','points','region_1','variety'])\n",
    "\n",
    "data_values=dfs[['_c0','description', 'designation','points','region_1','variety']].values.tolist()\n",
    "data_coulumns=['_c0','description', 'designation','points','region_1','variety']\n",
    "\n",
    "#将pandas.DataFrame转为spark.dataFrame，需要转数据和列名\n",
    "df_spark = spark.createDataFrame(data_values,data_coulumns)\n",
    "\n",
    "df_rdd = df_spark.rdd\n",
    "\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "\n",
    "# remove all non letter characters\n",
    "keyAndListOfWords1 = df_rdd.filter(lambda x :is_number(x[0]) is True and x[5] is not None and x[3] is not None\n",
    "                                  and is_number(x[3]) is True)\n",
    "\n",
    "keyAndListOfWords2 = keyAndListOfWords1.map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split(),x[2],x[3], x[4],x[5]))\n",
    "\n",
    "keyAndListOfWords = keyAndListOfWords2.map(lambda x : (x[0],[word for word in x[1] if word not in stopwords_set],x[2],\n",
    "                                                       tranfer_label(float(x[3])),x[4],x[5]))\n",
    "\n",
    "numberOfDocs = keyAndListOfWords.count()\n",
    "\n",
    "allWords = keyAndListOfWords.flatMap(lambda x: ((i,1) for i in x[1]))\n",
    "\n",
    "allCounts = allWords.reduceByKey(lambda x,y: x+y)\n",
    "allCounts.collect()\n",
    "topWords = allCounts.top(f,key = lambda x: x[1])\n",
    "\n",
    "topWordsK = sc.parallelize(range(f))\n",
    "\n",
    "dictionary = topWordsK.map (lambda x : (topWords[x][0], x))\n",
    "print(\"Word Postions in our Feature Matrix. Last 20 words in 20k positions: \", dictionary.top(20, lambda x : x[1]))\n",
    "\n",
    "dictionary.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[44] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "allWordsWithDocID = keyAndListOfWords.flatMap(lambda x: ((j, x[0]) for j in x[1]))\n",
    "\n",
    "allDictionaryWords = dictionary.join(allWordsWithDocID)\n",
    "\n",
    "justDocAndPos = allDictionaryWords.map(lambda x: (x[1][1], x[1][0]))\n",
    "\n",
    "allDictionaryWordsInEachDoc = justDocAndPos.groupByKey()\n",
    "\n",
    "allDocsAsNumpyArrays = allDictionaryWordsInEachDoc.map(lambda x: (x[0], buildArray(x[1])))\n",
    "\n",
    "\n",
    "\n",
    "zeroOrOne = allDocsAsNumpyArrays.map(lambda x: (x[0], np.where(x[1] > 0, 1, 0)))\n",
    "dfArray = zeroOrOne.reduce(lambda x1, x2: (\"\", np.add(x1[1], x2[1])))[1]\n",
    "\n",
    "multiplier = np.full(f, numberOfDocs)\n",
    "\n",
    "\n",
    "idfArray = np.log(np.divide(np.full(f, numberOfDocs), dfArray + 1))\n",
    "\n",
    "allDocsAsNumpyArraysTFidf = allDocsAsNumpyArrays.map(lambda x: (x[0], np.multiply(x[1], idfArray)))\n",
    "\n",
    "allDocsAsNumpyArraysTFidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('45850',\n",
       "  (array([0.        , 0.05990673, 0.        , ..., 0.        , 0.        ,\n",
       "          0.        ]),\n",
       "   0))]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labe_rdd = keyAndListOfWords.map(lambda x:(x[0],x[3]))\n",
    "\n",
    "a = allDocsAsNumpyArraysTFidf.join(labe_rdd)\n",
    "a.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the data costs 74.39946484565735seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "b = a.map(lambda x:(x[1][1],x[1][0],random.randint(1,3)))\n",
    "\n",
    "train = b.filter(lambda x: x[2] == 1 or x[2] == 2).map(lambda x: LabeledPoint(x[0],x[1]))\n",
    "\n",
    "\n",
    "stop = time()\n",
    "time1 = stop - start\n",
    "print('reading the data costs '+str(time1) + \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training the data costs 10907.47807598114seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "model = LogisticRegressionWithLBFGS.train(train,iterations=300,numClasses = 2)\n",
    "stop = time()\n",
    "time2 = stop - start\n",
    "print('training the data costs '+str(time2) + \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training the data costs 1208.0691299438477seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "model1 = LogisticRegressionWithSGD.train(train,iterations=300)\n",
    "stop = time()\n",
    "time2 = stop - start\n",
    "print('training the data costs '+str(time2) + \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training the data costs 2862.878396987915seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "model1 = SVMWithSGD.train(train,iterations = 500)\n",
    "stop = time()\n",
    "time2 = stop - start\n",
    "print('training the data costs '+str(time2) + \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training the data costs 57.928303718566895seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time()\n",
    "\n",
    "model2 = NaiveBayes.train(train)\n",
    "stop = time()\n",
    "time2 = stop - start\n",
    "print('training the data costs '+str(time2) + \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.81      0.82     17207\n",
      "    positive       0.88      0.89      0.88     26058\n",
      "\n",
      "    accuracy                           0.86     43265\n",
      "   macro avg       0.85      0.85      0.85     43265\n",
      "weighted avg       0.86      0.86      0.86     43265\n",
      "\n",
      "0.8584043474204661\n",
      "[[13958  3249]\n",
      " [ 2865 23193]]\n",
      "testing the data costs 19.40751314163208seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "def predict(x):\n",
    "    return model.predict(x)\n",
    "\n",
    "test = b.filter(lambda x: x[2] == 3)\n",
    "\n",
    "result = test.map(lambda x: (x[0],model.predict(x[1]))).collect()\n",
    "y=[]\n",
    "prediction =[]\n",
    "for i in result:\n",
    "    y.append(i[0])\n",
    "    prediction.append(i[1])\n",
    "# result_rf = test_rf.map(lambda x: (x[0],int(model1.predict(x[1]))))\n",
    "# result_rf.take(1)\n",
    "# # In[63]:\n",
    "\n",
    "\n",
    "f1 = f1_score(y, prediction, average='weighted')\n",
    "a = confusion_matrix(y, prediction)\n",
    "print(metrics.classification_report(y,prediction,target_names = ['negative','positive']))\n",
    "print(f1)\n",
    "print(a)\n",
    "\n",
    "stop = time()\n",
    "time3 = stop - start\n",
    "print('testing the data costs '+str(time3) + \"seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.98      0.08      0.15     17165\n",
      "    positive       0.62      1.00      0.77     26105\n",
      "\n",
      "    accuracy                           0.64     43270\n",
      "   macro avg       0.80      0.54      0.46     43270\n",
      "weighted avg       0.77      0.64      0.52     43270\n",
      "\n",
      "0.5244351316460362\n",
      "[[ 1435 15730]\n",
      " [   28 26077]]\n",
      "testing the data costs 21.888066053390503seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "def predict(x):\n",
    "    return model1.predict(x)\n",
    "\n",
    "test = b.filter(lambda x: x[2] == 3)\n",
    "\n",
    "result = test.map(lambda x: (x[0],model1.predict(x[1]))).collect()\n",
    "y=[]\n",
    "prediction =[]\n",
    "for i in result:\n",
    "    y.append(i[0])\n",
    "    prediction.append(i[1])\n",
    "# result_rf = test_rf.map(lambda x: (x[0],int(model1.predict(x[1]))))\n",
    "# result_rf.take(1)\n",
    "# # In[63]:\n",
    "\n",
    "\n",
    "f1 = f1_score(y, prediction, average='weighted')\n",
    "a = confusion_matrix(y, prediction)\n",
    "print(metrics.classification_report(y,prediction,target_names = ['negative','positive']))\n",
    "\n",
    "print(f1)\n",
    "print(a)\n",
    "\n",
    "stop = time()\n",
    "time3 = stop - start\n",
    "print('testing the data costs '+str(time3) + \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.61      0.71     17134\n",
      "    positive       0.79      0.92      0.85     26381\n",
      "\n",
      "    accuracy                           0.80     43515\n",
      "   macro avg       0.81      0.77      0.78     43515\n",
      "weighted avg       0.81      0.80      0.79     43515\n",
      "\n",
      "0.7938695548600613\n",
      "[[10496  6638]\n",
      " [ 2005 24376]]\n",
      "testing the data costs 29.95748805999756seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "def predict(x):\n",
    "    return model2.predict(x)\n",
    "\n",
    "test = b.filter(lambda x: x[2] == 3)\n",
    "\n",
    "result = test.map(lambda x: (x[0],model2.predict(x[1]))).collect()\n",
    "y=[]\n",
    "prediction =[]\n",
    "for i in result:\n",
    "    y.append(i[0])\n",
    "    prediction.append(i[1])\n",
    "# result_rf = test_rf.map(lambda x: (x[0],int(model1.predict(x[1]))))\n",
    "# result_rf.take(1)\n",
    "# # In[63]:\n",
    "\n",
    "\n",
    "f1 = f1_score(y, prediction, average='weighted')\n",
    "a = confusion_matrix(y, prediction)\n",
    "print(metrics.classification_report(y,prediction,target_names = ['negative','positive']))\n",
    "\n",
    "print(f1)\n",
    "print(a)\n",
    "\n",
    "stop = time()\n",
    "time3 = stop - start\n",
    "print('testing the data costs '+str(time3) + \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
